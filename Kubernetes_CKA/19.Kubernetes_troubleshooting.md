# Troubleshooting Your K8s Cluster
# Kube API Server
If the K8s API server is down, you will not be able to use kubectl to interact with the cluster.
Assuming your kubeconfig is set up correctly, this may mean the API server is down.
Possible fixes:
Make sure the docker and kubelet services are up and running on your control plane node(s)

# Checking Node Status
Check the status of your nodes to see if any of them are experiencing issues.
Use kubectl get nodes to see the overall status of each node.
    $ kubectl get nodes
Use kubectl describe node to get more information on any nodes that are not in the READY state.
    $ kubectl describe node node name
If a node is having problems, it may be because a service is down on that node.
Each node runs the kubelet and container runtime (i.e. Docker) services.
1. View the status of a service.
    $ systemctl status kubelet
2. Start a stopped service.
    $ systemctl start kubelet
3. Enable a service so it starts automatically on system startup. 
    $ systemctl enable kubelet

# Checking System Pods
In a kubeadm cluster, several K8s components run as pods in the kube-system namespace.
Check the status of these components with kubectl get pods and kubectl describe pod.
    $ kubectl get pods -n kube-system
    $ kubectl describe pod podname -n kube-system

# Service Logs 
You can check the logs for K8s-related services on each node using journalctl.
    $ sudo journalctl -u kubelet
    $ sudo journalctl -u docker

# Cluster Component Logs 
The Kubernetes cluster components have log output redirected to /var/log. For example:
    /var/log/kube-apiserver.log
    /var/log/kube-scheduler.log
    /var/log/kube-controller-manager.log
Note that these log files may not appear for kubeadm clusters, since some components run inside containers. In that case, you can access them with kubectl logs.

# Troubleshooting the Applications
# Checking Pod Status
You can see a Pod’s status with kubectl get pods.
    $ kubectl get pods
    $ kubectl describe pod podname

# Running Commands Inside Containers
If you need to troubleshoot what is going on inside a container, you can execute commands within the container with kubectl exec.
    $ kubectl exec podname -c containername -- command
Note that you cannot use kubectl exec to run any software that is not present within the container

# Container Logging
K8s containers maintain logs, which you can use to gain insight into what is going on within the container.
A container’s log contains everything written to the standard output (stdout) and error (stderr) streams by the container process.

# kubectl logs
Use the kubectl logs command to view a container’s logs.
    $ kubectl logs podname -c containername

# Troubleshooting K8s Networking Issues
# kube-proxy and DNS
In addition to checking on your K8s networking plugin, it may be a good idea to look at kube-proxy and the K8s DNS if you are experiencing stat within the K8s cluster network.
In a kubeadm cluster, the K8s DNS and kube-proxy run as Pods in the kube-system namespace.

# netshoot
Tip: You can run a container in the cluster that you can use to run commands to test and gather information about network functionality.
The nicolaka/netshoot image is a great tool for this. This image contains a variety of networking exploration and troubleshooting tools.
Create a container running this image, and then use kubectl exec to explore away!

# Troubleshooting Summary
• Troubleshooting Your K8s Cluster
    $ systemctl status kubelet
    $ kubectl get nodes
    $ kubectl get pods -n kube-system
• Checking Cluster and Node Logs (shift+j jump to end)
    $ sudo journalctl -u kubelet
    $ sudo journalctl -u docker
    $ kubectl logs -n kube-system <SYSTEM_POD>
• Troubleshooting Your Applications
    $ kubectl get pods
    $ kubectl descrobe pod <POD_NAME>
    $ kubectl exec <POD_NAME> -c <container_name> -- command
                                                -- stdin --tty -- /bin/sh to start interactive shell for example
• Checking Container Logs
**
# Troubleshooting K8s Networking Issues
vi nginx-netshoot.yml
apiVersion: v1
kind: Pod
metadata:
name: nginx-netshoot
labels:
app: nginx-netshoot
spec:
containers:
- name: nginx
image: nginx:1.19.1
---
apiVersion: v1
kind: Service
metadata:
name: svc-netshoot
spec:
type: ClusterIP
selector:
app: nginx-netshoot
ports:
- protocol: TCP
port: 80
targetPort: 80
kubectl create -f nginx-netshoot.yml
Create a Pod running the netshoot image in a container:
vi netshoot.yml
apiVersion: v1
kind: Pod
metadata:
name: netshoot
spec:
containers:
- name: netshoot
image: nicolaka/netshoot
command: ['sh', '-c', 'while true; do sleep 5; done']
kubectl create -f netshoot.yml
Open an interactive shell to the netshoot container:
kubectl exec --stdin --tty netshoot -- /bin/sh
curl svc-netshoot
ping svc-netshoot
nslookup svc-netshoot

ref: 
https://github.com/nicolaka/netshoot
https://kubernetes.io/docs/tasks/administer-cluster/dns-debugging-resolution/
https://kubernetes.io/docs/tasks/debug/debug-application/debug-service/