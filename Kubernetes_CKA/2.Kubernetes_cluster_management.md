# Performing a Kubernetes Upgrade with kubeadm
Introduction
When you are managing Kubernetes in the real world, it is essential that you are able to keep your cluster up to date. This lab will allow you to practice the process of upgrading a Kubernetes cluster to a newer Kubernetes version using kubeadm. This will ensure you are comfortable with the upgrade process and ready to manage real-world Kubernetes clusters.

1. Upgrade the Control Plane
Upgrade kubeadm:

[cloud_user@k8s-control]$ sudo apt-get update && \
sudo apt-get install -y --allow-change-held-packages kubeadm=1.27.2-00
Make sure it upgraded correctly:

[cloud_user@k8s-control]$ kubeadm version
2. Drain the control plane node:

[cloud_user@k8s-control]$ kubectl drain k8s-control --ignore-daemonsets
3. Plan the upgrade:

[cloud_user@k8s-control]$ sudo kubeadm upgrade plan v1.27.2
4. Upgrade the control plane components:

[cloud_user@k8s-control]$ sudo kubeadm upgrade apply v1.27.2
5. Upgrade kubelet and kubectl on the control plane node:

[cloud_user@k8s-control]$ sudo apt-get update && \
sudo apt-get install -y --allow-change-held-packages kubelet=1.27.2-00 kubectl=1.27.2-00
6. Restart kubelet:

[cloud_user@k8s-control]$ sudo systemctl daemon-reload
[cloud_user@k8s-control]$ sudo systemctl restart kubelet
7. Uncordon the control plane node:

[cloud_user@k8s-control]$ kubectl uncordon k8s-control
8. Verify the control plane is working:

[cloud_user@k8s-control]$ kubectl get nodes
If it shows a NotReady status, run the command again after a minute or so. It should become Ready.

9. Upgrade the Worker Nodes
Note: In a real-world scenario, you should not perform upgrades on all worker nodes at the same time. Make sure enough nodes are available at any given time to provide uninterrupted service.

10. Worker Node 1
Run the following on the control plane node to drain worker node 1:

[cloud_user@k8s-control]$ kubectl drain k8s-worker1 --ignore-daemonsets --force
You may get an error message that certain pods couldn't be deleted, which is fine.

11. Upgrade kubeadm on worker node 1:

[cloud_user@k8s-worker1]$ sudo apt-get update && \
sudo apt-get install -y --allow-change-held-packages kubeadm=1.27.2-00
[cloud_user@k8s-worker1]$ kubeadm version
Back on worker node 1, upgrade the kubelet configuration on the worker node:

[cloud_user@k8s-worker1]$ sudo kubeadm upgrade node
Upgrade kubelet and kubectl on worker node 1:

[cloud_user@k8s-worker1]$ sudo apt-get update && \
sudo apt-get install -y --allow-change-held-packages kubelet=1.27.2-00 kubectl=1.27.2-00
12. Restart kubelet:

[cloud_user@k8s-worker1]$ sudo systemctl daemon-reload
[cloud_user@k8s-worker1]$ sudo systemctl restart kubelet
From the control plane node, uncordon worker node 1:

[cloud_user@k8s-control]$ kubectl uncordon k8s-worker1
13. Worker Node 2
From the control plane node, drain worker node 2:

[cloud_user@k8s-control]$ kubectl drain k8s-worker2 --ignore-daemonsets --force
In a new terminal window, log in to worker node 2:

ssh cloud_user@<WORKER_2_PUBLIC_IP_ADDRESS>
14. Upgrade kubeadm:

[cloud_user@k8s-worker2]$ sudo apt-get update && \
sudo apt-get install -y --allow-change-held-packages kubeadm=1.27.2-00
[cloud_user@k8s-worker2]$ kubeadm version
Back on worker node 2, perform the upgrade:

[cloud_user@k8s-worker2]$ sudo kubeadm upgrade node
[cloud_user@k8s-worker2]$ sudo apt-get update && \
sudo apt-get install -y --allow-change-held-packages kubelet=1.27.2-00 kubectl=1.27.2-00
[cloud_user@k8s-worker2]$ sudo systemctl daemon-reload
[cloud_user@k8s-worker2]$ sudo systemctl restart kubelet
From the control plane node, uncordon worker node 2:

[cloud_user@k8s-control]$ kubectl uncordon k8s-worker2
Still in the control plane node, verify the cluster is upgraded and working:

[cloud_user@k8s-control]$ kubectl get nodes
If they show a NotReady status, run the command again after a minute or so. They should become Ready.

# Backing up and Restoring Kubernetes Data in etcd
Backups are an important part of any resilient system. Kubernetes is no exception. In this lab, you will have the opportunity to practice your skills by backing up and restoring a Kubernetes cluster state stored in etcd. This will help you get comfortable with the steps involved in backing up Kubernetes data.
etcd is the backend  data storage solution for kubernetes, objects, apps, configuration, code....
using the etcdctl snapshot save command to back up the data and if you want to restore you use the restore command

# Guide
Log in to the provided lab server using the credentials provided:

0. Check data
ETCDCTL_API=3 etcdctl get cluster.name \
  --endpoints=https://10.0.1.101:2379 \
  --cacert=/home/cloud_user/etcd-certs/etcd-ca.pem \
  --cert=/home/cloud_user/etcd-certs/etcd-server.crt \
  --key=/home/cloud_user/etcd-certs/etcd-server.key
The returned value should be beebox.

1. Back up etcd using etcdctl and the provided etcd certificates:

ETCDCTL_API=3 etcdctl snapshot save /home/cloud_user/etcd_backup.db \
  --endpoints=https://10.0.1.101:2379 \
  --cacert=/home/cloud_user/etcd-certs/etcd-ca.pem \
  --cert=/home/cloud_user/etcd-certs/etcd-server.crt \
  --key=/home/cloud_user/etcd-certs/etcd-server.key
2. Reset etcd by removing all existing etcd data:

sudo systemctl stop etcd
sudo rm -rf /var/lib/etcd
3. Restore the etcd Data from the Backup
Restore the etcd data from the backup (this command spins up a temporary etcd cluster, saving the data from the backup file to a new data directory in the same location where the previous data directory was):

sudo ETCDCTL_API=3 etcdctl snapshot restore /home/cloud_user/etcd_backup.db \
  --initial-cluster etcd-restore=https://10.0.1.101:2380 \
  --initial-advertise-peer-urls https://10.0.1.101:2380 \
  --name etcd-restore \
  --data-dir /var/lib/etcd
4. Set ownership on the new data directory:

sudo chown -R etcd:etcd /var/lib/etcd
5. Start etcd:

sudo systemctl start etcd
6. Verify the restored data is present by looking up the value for the key cluster.name again:

ETCDCTL_API=3 etcdctl get cluster.name \
  --endpoints=https://10.0.1.101:2379 \
  --cacert=/home/cloud_user/etcd-certs/etcd-ca.pem \
  --cert=/home/cloud_user/etcd-certs/etcd-server.crt \
  --key=/home/cloud_user/etcd-certs/etcd-server.key
The returned value should be beebox.
