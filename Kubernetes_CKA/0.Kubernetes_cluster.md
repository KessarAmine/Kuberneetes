# Building a Kubernetes 1.27 Cluster with kubeadm (COURSE NOTES)

kubernetes cluster is a layer that has multiple servers which has multiple containers

# Kubernetes Features
container orchestration: duynamically manage containers throught different host systems
Reliabilty: build reliable, self-healing, scalable apps
automation: automate the management of the containers apps

# Architectural overview
control plan: 
runs and manages and controls kubernetes cluster
    kube-api-server: interaction with kubernetes cluster
    etcd: provied the backend data store, destributed, relates to the state of the cluster
    kube-scheduler: selecting an available node in the cluster to run the container
    kube-controller-manager: runs a collection of multiple controller utilites in a single process
    cloud-controller-manager: interfaces to interact your kubernetes with cloud platforms / services

worker nodes (kubernetes nodes):
the machine where the container managed by the cluster is running, they commincate with CP
    kube-proxy: network proxy that provides netowrking communication between containers and services in the cluster
    kubelet: agent run on each node, ensure that everything is running accordanly to the CP, reports the status and infos about the containers back to the CP
    container runtime: seperate software of the kubernetes responsible for running containers on the machines, (ie. docker and containerd)

# Building a kubernetes cluster
kubeadm: simplify the process of setting up your kubernetes cluster
tips:
all configuration are loaded up once start up, to apply them cahnges in the configuration in system level we use sudo sysctl --system to reload them
containerd.conf is found at /etc/modules-load.d/
system level configurations can be found at /etc/sysctl.d/
1. set hosnames with hostnamectl set-hostname on each node
2. go to the hosts file (etc/hosts) and add each node wiht its ip_adress and its hostname, do this for all nodes
3. enable kernel moduels loading when containerd start running use tee to containerd.conf add them (overlay | br_netfilter) then use modprobe to directly enable them without the need for restart
4. config the network system variables inside the system configuration file : 99-kubernetes-cri.conf as follow: 
    cat << EOF | sudo tee /etc/sysctl.d/99-kuberenetes-cri.conf
        net.bridge.bridge-nf-call-iptables = 1
        net.piv4.ip_forward = 1
        net.bridge.bridge-nf-call-ip6tables = 1
5. install the runtime containerd 
6. create a containerd configuration file udner /etc/containerd using command sudo containerd config default | /*pipe the output of file generated into*/ sudo tee /etc/containerd/config.toml /*to make sure cintainerd is using that file we do*/ sudo systemctl restart containerd
7. install kubernetes packages, swap has to be disabled in all servers so we do "sudo swapoff -a" and we have to make sure "apt-transport-https" and "curl" are installed and some other necessary steps
8. install kubelet, kubectl, kubeadm all of them same versions would be better
9. stop automatic upgrade on those packages so use "apt-mark hold" command on them  
10. initialize cluster /*this is done only in contral panel*/
    sudo kubeadm init --pod-network-cidr 192.168.0.0/16 --kubernetes-version 1.27.0
    /*--pod-network-cidr 192.168.0.0/16 using our private ip in the hosts of the contral panel we set the range of ip adresses for our internal pod netwok communications*/ 
11. setup kubeconfig, a file to authaunticate and interact with the cluster using kubectl commands, as the output of the previous initzialtion mentions
12. test the clsuter intizialitions and kubeconfig setup by running a kubectl command
    kubectl get nodes
    /*should get the control panel node status*/
13. to add worker nodes to the cluster their should be a network plugin to provide it for us there are multiple ones, we will add colico.yml manifest file that will do the job for us
    kubectl applu -f <URL>
14. Join worker nodes to the cluster, for that we need a join command with a token
    kubeadm token create --print-join-command
15. sudo <PREVIOUS_OUTPUT> in the worker nodes should be enogh to join the worker nodes to the cluster
16. test again kubectl get nodes, all worker nodes and control panel nodes should be shown, worker nodes need couple of minutes to get READY status

# Building a Kubernetes 1.27 Cluster with kubeadm (LAB NOTES)

Create the configuration file for containerd:

cat <<EOF | sudo tee /etc/modules-load.d/containerd.conf
overlay
br_netfilter
EOF
Load the modules:

sudo modprobe overlay
sudo modprobe br_netfilter
Set the system configurations for Kubernetes networking:

cat <<EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-ip6tables = 1
EOF
Apply the new settings:

sudo sysctl --system
Install containerd:

sudo apt-get update && sudo apt-get install -y containerd.io
Create the default configuration file for containerd:

sudo mkdir -p /etc/containerd
Generate the default containerd configuration, and save it to the newly created default file:

sudo containerd config default | sudo tee /etc/containerd/config.toml
Restart containerd to ensure the new configuration file is used:

sudo systemctl restart containerd
Verify that containerd is running:

sudo systemctl status containerd
Disable swap:

sudo swapoff -a
Install the dependency packages:

sudo apt-get update && sudo apt-get install -y apt-transport-https curl
Download and add the GPG key:

curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
Add Kubernetes to the repository list:

cat <<EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list
deb https://apt.kubernetes.io/ kubernetes-xenial main
EOF
Update the package listings:

sudo apt-get update
Install Kubernetes packages:

Note: If you get a dpkg lock message, just wait a minute or two before trying the command again.

sudo apt-get install -y kubelet=1.27.0-00 kubeadm=1.27.0-00 kubectl=1.27.0-00
Turn off automatic updates:

sudo apt-mark hold kubelet kubeadm kubectl
Log in to both worker nodes to perform the previous steps.

Initialize the Cluster
Initialize the Kubernetes cluster on the control plane node using kubeadm:

sudo kubeadm init --pod-network-cidr 192.168.0.0/16 --kubernetes-version 1.27.0
Set kubectl access:

mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
Test access to the cluster:

kubectl get nodes
Install the Calico Network Add-On
On the control plane node, install Calico Networking:

kubectl apply -f https://raw.githubusercontent.com/projectcalico/calico/v3.25.0/manifests/calico.yaml
Check the status of the control plane node:

kubectl get nodes
Join the Worker Nodes to the Cluster
In the control plane node, create the token and copy the kubeadm join command:

kubeadm token create --print-join-command
Note: This output will be used as the next command for the worker nodes.

Copy the full output from the previous command used in the control plane node. This command starts with kubeadm join.

In both worker nodes, paste the full kubeadm join command to join the cluster. Use sudo to run it as root:

sudo kubeadm join...
In the control plane node, view the cluster status:

kubectl get nodes
Note: You may have to wait a few moments to allow all nodes to become ready.

Conclusion
Congratulations â€” you've completed this hands-on lab!